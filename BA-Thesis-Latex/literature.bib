

@article{Dorr2015,
abstract = {In our paper we investigate the use of qualitative spatial representations (QSR) about relative direction and distance for shape representation. Our new approach has the advantage that we can generate prototypical shapes from our abstract representation in first-order predicate calculus. Using the conceptual neighborhood which is an established concept in QSR we can directly establish a conceptual neighborhood between shapes that translates into a similarity metric for shapes. We apply this similarity measure to a challenging computer vision problem and achieve promising first results.},
author = {Dorr, Christopher H. and Latecki, Longin Jan and Moratz, Reinhard},
doi = {10.1007/978-3-319-23374-1_7},
file = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester_SS_2023/BA Vorbereitung/Moratz_Thema/Literatur/Shape Similarity/Shape Similarity Based on the Qualitative Spatial Reasoning Calculus eOPRAm.pdf:pdf},
isbn = {9783319233734},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Computer vision,Qualitative shape representation,Qualitative spatial reasoning},
mendeley-groups = {BA Thesis},
pages = {130--150},
title = {{Shape similarity based on the qualitative spatial reasoning calculus eOPRAm}},
volume = {9368},
year = {2015}
}

@article{Lin2021,
abstract = {In recent years, vehicle detection and classification have become essential tasks of intelligent transportation systems, and real-time, accurate vehicle detection from image and video data for traffic monitoring remains challenging. The most noteworthy challenges are real-time system operation to accurately locate and classify vehicles in traffic flows and working around total occlusions that hinder vehicle tracking. For real-time traffic monitoring, we present a traffic monitoring approach that overcomes the abovementioned challenges by employing convolutional neural networks that utilize You Only Look Once (YOLO). A real-time traffic monitoring system has been developed, and it has attracted significant attention from traffic management departments. Digitally processing and analyzing these videos in real time is crucial for extracting reliable data on traffic flow. Therefore, this study presents a real-time traffic monitoring system based on a virtual detection zone, Gaussian mixture model (GMM), and YOLO to increase the vehicle counting and classification efficiency. GMM and a virtual detection zone are used for vehicle counting, and YOLO is used to classify vehicles. Moreover, the distance and time traveled by a vehicle are used to estimate the speed of the vehicle. In this study, the Montevideo Audio and Video Dataset (MAVD), the GARM Road-Traffic Monitoring data set (GRAM-RTM), and our collection data sets are used to verify the proposed method. Experimental results indicate that the proposed method with YOLOv4 achieved the highest classification accuracy of 98.91\% and 99.5\% in MAVD and GRAM-RTM data sets, respectively. Moreover, the proposed method with YOLOv4 also achieves the highest classification accuracy of 99.1\%, 98.6\%, and 98\% in daytime, night time, and rainy day, respectively. In addition, the average absolute percentage error of vehicle speed estimation with the proposed method is about 7.6\%.},
author = {Lin, Cheng Jian and Jeng, Shiou Yun and Lioa, Hong Wei},
doi = {10.1155/2021/1577614},
file = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester_SS_2023/BA Vorbereitung/Moratz_Thema/Literatur/Forschungskontext Verkehrsmonitoring/A Real-Time Vehicle Counting, Speed Estimation, and Classification System Based on....pdf:pdf},
issn = {15635147},
journal = {Mathematical Problems in Engineering},
mendeley-groups = {BA Thesis},
title = {{A Real-Time Vehicle Counting, Speed Estimation, and Classification System Based on Virtual Detection Zone and YOLO}},
volume = {2021},
year = {2021}
}

@article{Al-qaness2021,
abstract = {The growing population in large cities is creating traffic management issues. The metropolis road network management also requires constant monitoring, timely expansion, and modernization. In order to handle road traffic issues, an intelligent traffic management solution is required. Intelligent monitoring of traffic involves the detection and tracking of vehicles on roads and highways. There are various sensors for collecting motion information, such as transport video detectors, microwave radars, infrared sensors, ultrasonic sensors, passive acoustic sensors, and others. In this paper, we present an intelligent video surveillance-based vehicle tracking system. The proposed system uses a combination of the neural network, image-based tracking, and You Only Look Once (YOLOv3) to track vehicles. We train the proposed system with different datasets. Moreover, we use real video sequences of road traffic to test the performance of the proposed system. The evaluation outcomes showed that the proposed system can detect, track, and count the vehicles with acceptable results in changing scenarios.},
author = {Al-qaness, Mohammed A.A. and Abbasi, Aaqif Afzaal and Fan, Hong and Ibrahim, Rehab Ali and Alsamhi, Saeed H. and Hawbani, Ammar},
doi = {10.1007/s00607-020-00869-8},
file = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester_SS_2023/BA Vorbereitung/Moratz_Thema/Literatur/Forschungskontext Verkehrsmonitoring/A improved YOLO based road traffic monitoring system.pdf:pdf},
isbn = {0060702000869},
issn = {14365057},
journal = {Computing},
keywords = {Computer vision,Intelligent traffic,Neural network,Traffic analysis,YOLOv3},
mendeley-groups = {BA Thesis},
number = {2},
pages = {211--230},
publisher = {Springer Vienna},
title = {{An improved YOLO-based road traffic monitoring system}},
url = {https://doi.org/10.1007/s00607-020-00869-8},
volume = {103},
year = {2021}
}

@misc{Lietmeyer2023,
author = {Lietmeyer, Timo},
mendeley-groups = {BA Thesis},
title = {{Shape-based object tracking with the Discrete Curve Evolution}},
url = {https://github.com/Timo123456789/shape_based_object_tracking_with_the_discrete_curve_evolution},
urldate = {2023-09-17},
year = {2023}
}


@misc{Zope2023,
author = {and Contributors, Zope Foundation},
mendeley-groups = {BA Thesis},
title = {{DateTime {\textperiodcentered} PyPI}},
url = {https://pypi.org/project/DateTime/#section-1},
urldate = {2023-09-17},
year = {2023}
}


@ARTICLE{Latecki2007,
  author={Bai, Xiang and Latecki, Longin Jan and Liu, Wen-yu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Skeleton Pruning by Contour Partitioning with Discrete Curve Evolution}, 
  year={2007},
  volume={29},
  number={3},
  pages={449-462},
  abstract={In this paper, we introduce a new skeleton pruning method based on contour partitioning. Any contour partition can be used, but the partitions obtained by discrete curve evolution (DCE) yield excellent results. The theoretical properties and the experiments presented demonstrate that obtained skeletons are in accord with human visual perception and stable, even in the presence of significant noise and shape variations, and have the same topology as the original skeletons. In particular, we have proven that the proposed approach never produces spurious branches, which are common when using the known skeleton pruning methods. Moreover, the proposed pruning method does not displace the skeleton points. Consequently, all skeleton points are centers of maximal disks. Again, many existing methods displace skeleton points in order to produces pruned skeletons},
  keywords={},
  doi={10.1109/TPAMI.2007.59},
  ISSN={1939-3539},
  month={March},}



@article{ZHENG201517,
title = {A robust channel network extraction method combining discrete curve evolution and the skeleton construction technique},
journal = {Advances in Water Resources},
volume = {83},
pages = {17-27},
year = {2015},
issn = {0309-1708},
doi = {https://doi.org/10.1016/j.advwatres.2015.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0309170815000962},
author = {Xianwei Zheng and Hanjiang Xiong and Jianya Gong and Linwei Yue},
keywords = {Contour lines, Channel network extraction, Delaunay triangulation, Discrete curve evolution, Skeleton construction},
abstract = {The automatic mapping of drainage networks from terrain representation has been an interesting topic in hydrological and geomorphological modeling. However, the existing methods often suffer from high sensitivity to terrain noise or lose significant stream branches and accurate channel paths. In this paper, we propose a contour-based framework in drainage network extraction. The proposed framework incorporates discrete curve evolution (DCE) to eliminate the noise influence by dynamically segmenting the contour lines (CLs) into valley bends, and to detect the valley feature points. The skeleton construction technique is then applied to distill more accurate channel paths in complex terrain. Finally, a linking step is undertaken to generate the channel network. The proposed method was tested on a series of elevation datasets, with varied resolution, region size, and local relief. The experiments verified that the proposed method can achieve highly accurate channel networks and is robust, even in regions with high-contrast relief, and/or in cases with significant terrain noise and irregularities.}
}


@incollection{Latecki2000a,
author = {Latecki, Longin Jan and DeMenthon, Daniel and Rosenfeld, Azriel},
doi = {10.1007/978-3-642-59802-9_52},
file = {:C\:/Users/timol/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Latecki, DeMenthon, Rosenfeld - 2000 - Automatic Extraction of Relevant Frames from Videos by Polygon Simplification.pdf:pdf},
mendeley-groups = {Proposal,BA Thesis},
pages = {412--419},
title = {{Automatic Extraction of Relevant Frames from Videos by Polygon Simplification}},
year = {2000}
}


@INPROCEEDINGS{Supot2007,
  author={Supot, Sookpotharom and Thanapong, Chaichana and Chuchart, Pintavirooj and Manas, Sangworasil},
  booktitle={2007 IEEE International Conference on Integration Technology}, 
  title={Segmentation of Magnetic Resonance Images Using Discrete Curve Evolution and Fuzzy Clustering}, 
  year={2007},
  volume={},
  number={},
  pages={697-700},
  abstract={The region clustering of a magnetic resonance imaging (MRI) image is more complicate than a computed topography (CT) image because a MRI image composes of three components such as Tl -weighted, T2-weighted, and proton density (PD) in each layer. However, the MRI images provide more detail than the CT images. Therefore, we propose a technique of the region clustering of MRI image by using fuzzy c-means (PCM). The fuzzy c-means algorithm is an iterative operation, that is very time-consuming and makes the algorithm impractical for using in image segmentation. To cope with this problem, the discrete curve evolution (DCE) technique is applied to find the actual cluster center to refine the initial value of the fuzzy c-means algorithm, which reduces the convergence time. In experimental results, the proposed technique provides the same segmentation accuracy as the fuzzy c-means technique. Moreover, this technique takes lower computational time comparing to the previous method.},
  keywords={},
  doi={10.1109/ICITECHNOLOGY.2007.4290409},
  ISSN={},
  month={March},}


@article{Latecki2000,
author = {Longin Jan Latecki and R. R. Ghadially and Rolf Lakaemper and Ulrich Eckhardt},
title = {{Continuity of discrete curve evolution}},
volume = {9},
journal = {Journal of Electronic Imaging},
number = {3},
publisher = {SPIE},
pages = {317 -- 326},
abstract = {},
keywords = {Image segmentation, Image processing, Digital image processing, Digital imaging, Feature extraction, Applied mathematics, CCD cameras, Computer vision technology, Image understanding, Machine vision},
year = {2000},
doi = {10.1117/1.482748},
URL = {https://doi.org/10.1117/1.482748}
}

@INPROCEEDINGS{Lai2016,
  author={Lai, Zhongyuan and Yao, Zhijun and Wang, Chun and Liang, Hui and Chen, Hongmei and Xia, Wu},
  booktitle={2016 Visual Communications and Image Processing (VCIP)}, 
  title={Fingertips detection and hand gesture recognition based on discrete curve evolution with a kinect sensor}, 
  year={2016},
  volume={},
  number={},
  pages={1-4},
  abstract={In this paper, we propose a novel method that can detect fingertips as well as recognize hand gestures. Firstly, we collect the hand curves with a Kinect sensor. Secondly, we detect fingertips based on the discrete curve evolution. Thirdly, we recognize hand gestures using evolved curves partitioned at the detected fingertips. Experimental results show that our method performs well in both fingertips detection and hand gesture recognition.},
  keywords={},
  doi={10.1109/VCIP.2016.7805464},
  ISSN={},
  month={Nov},}


@article{Latecki1998,
author = {Latecki, Longin Jan and Lak{\"{a}}mper, Rolf},
doi = {10.1007/978-3-642-72282-0_7},
file = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester_SS_2023/BA Vorbereitung/Moratz_Thema/Literatur/02 Theoretischer Hintergrund/DCE Method/12. Discrete Aproach to Curve Evolution.pdf:pdf},
mendeley-groups = {BA Thesis},
pages = {85--92},
title = {{Discrete Approach to Curve Evolution}},
year = {1998}
}



@article{Hoare1962QS,
    author = {Hoare, C. A. R.},
    title = "{Quicksort}",
    journal = {The Computer Journal},
    volume = {5},
    number = {1},
    pages = {10-16},
    year = {1962},
    month = {01},
    abstract = "{A description is given of a new method of sorting in the random-access store of a computer. The method compares very favourably with other known methods in speed, in economy of storage, and in ease of programming. Certain refinements of the method, which may be useful in the optimization of inner loops, are described in the second part of the paper.}",
    issn = {0010-4620},
    doi = {10.1093/comjnl/5.1.10},
    url = {https://doi.org/10.1093/comjnl/5.1.10},
    eprint = {https://academic.oup.com/comjnl/article-pdf/5/1/10/1111445/050010.pdf},
}





@article{Dorr2017,
	title        = {Towards Applying the OPRA Theory to Shape Similarity},
	author       = {Dorr, Christopher H. and Moratz, Reinhard},
	year         = 2017,
	number       = {May 2017},
	url          = {http://arxiv.org/abs/1705.02653},
	abstract     = {The motivation for using qualitative shape descriptions is as follows: qualitative shape descriptions can implicitly act as a schema for measuring the similarity of shapes, which has the potential to be cognitively adequate. Then, shapes which are similar to each other would also be similar for a pattern recognition algorithm. There is substantial work in pattern recognition and computer vision dealing with shape similarity. Here with our approach to qualitative shape descriptions and shape similarity, the focus is on achieving a representation using only simple predicates that a human could even apply without computer support.},
	archiveprefix = {arXiv},
	arxivid      = {1705.02653},
	eprint       = {1705.02653},
	file         = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester_SS_2023/BA Vorbereitung/Moratz_Thema/Literatur/Towards_Applying_the_OPRA_Theory_to_Shape_Similari.pdf:pdf},
	mendeley-groups = {Proposal,Proposal/Small}
}
@incollection{Barkowsky2000,
	title        = {Schematizing Maps: Simplification of Geographic Shape by Discrete Curve Evolution},
	author       = {Barkowsky, Thomas and Latecki, Longin Jan and Richter, Kai -Florian},
	year         = 2000,
	volume       = 8,
	pages        = {41--53},
	doi          = {10.1007/3-540-45460-8\_4},
	url          = {http://link.springer.com/10.1007/3-540-45460-8\_4},
	abstract     = {Shape simplification in map-like representations is used for two reasons: either to abstract from irrelevant detail to reduce a map user's cognitive load, or to simplify information when a map of a smaller scale is derived from a detailed reference map. We present a method for abstracting simplified cartographic representations from more accurate spatial data. First, the employed method of discrete curve evolution developed for simplifying perceptual shape characteristics is explained. Specific problems of applying the method to cartographic data are elaborated. An algorithm is presented, which on the one hand simplifies spatial data up to a degree of abstraction intended by the user; and which on the other hand does not violate local spatial ordering between (elements of) cartographic entities, since local arrangement of entities is assumed to be an important spatial knowledge characteristic. The operation of the implemented method is demonstrated using two different examples of cartographic data.},
	file         = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester\_SS\_2023/BA Vorbereitung/Moratz\_Thema/Literatur/Schematizing\_Maps\_Simplification\_of\_Geog.pdf:pdf},
	mendeley-groups = {Proposal,Proposal/Small}
}
@article{Latecki1999d,
	title        = {Convexity Rule for Shape Decomposition Based on Discrete Contour Evolution},
	author       = {Latecki, Longin Jan and Lak{\"{a}}mper, Rolf},
	year         = 1999,
	journal      = {Computer Vision and Image Understanding},
	volume       = 73,
	number       = 3,
	pages        = {441--454},
	doi          = {10.1006/cviu.1998.0738},
	issn         = 10773142,
	abstract     = {We concentrate here on decomposition of 2D objects into meaningfulparts of visual form, orvisual parts. It is a simple observation that convex parts of objects determine visual parts. However, the problem is that many significant visual parts are not convex, since a visual part may have concavities. We solve this problem by identifying convex parts at different stages of a proposed contour evolution method in which significant visual parts will become convex object parts at higher stages of the evolution. We obtain a novel rule for decomposition of 2D objects into visual parts, called the hierarchical convexity rule, which states that visual parts are enclosed by maximal convex (with respect to the object) boundary arcs at different stages of the contour evolution. This rule determines not only parts of boundary curves but directly the visual parts of objects. Moreover, the stages of the evolution hierarchy induce a hierarchical structure of the visual parts. The more advanced the stage of contour evolution, the more significant is the shape contribution of the obtained visual parts. {\textcopyright} 1999 Academic Press.},
	file         = {:C\:/Users/timol/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Latecki, Lak{\"{a}}mper - 1999 - Convexity Rule for Shape Decomposition Based on Discrete Contour Evolution.pdf:pdf},
	keywords     = {Digital curves,Digital geometry,Digital straight line segments,Discrete curve evolution,Shape hierarchy,Total curvature,Visual parts},
	mendeley-groups = {Proposal,Proposal/Small}
}

@article{Latecki1999a,
abstract = {We concentrate here on decomposition of 2D objects into meaningfulparts of visual form, orvisual parts. It is a simple observation that convex parts of objects determine visual parts. However, the problem is that many significant visual parts are not convex, since a visual part may have concavities. We solve this problem by identifying convex parts at different stages of a proposed contour evolution method in which significant visual parts will become convex object parts at higher stages of the evolution. We obtain a novel rule for decomposition of 2D objects into visual parts, called the hierarchical convexity rule, which states that visual parts are enclosed by maximal convex (with respect to the object) boundary arcs at different stages of the contour evolution. This rule determines not only parts of boundary curves but directly the visual parts of objects. Moreover, the stages of the evolution hierarchy induce a hierarchical structure of the visual parts. The more advanced the stage of contour evolution, the more significant is the shape contribution of the obtained visual parts. {\textcopyright} 1999 Academic Press.},
author = {Latecki, Longin Jan and Lak{\"{a}}mper, Rolf},
doi = {10.1006/cviu.1998.0738},
file = {:C\:/Users/timol/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Latecki, Lak{\"{a}}mper - 1999 - Convexity Rule for Shape Decomposition Based on Discrete Contour Evolution.pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Digital curves,Digital geometry,Digital straight line segments,Discrete curve evolution,Shape hierarchy,Total curvature,Visual parts},
mendeley-groups = {Proposal,Proposal/Small,BA Thesis},
number = {3},
pages = {441--454},
title = {{Convexity Rule for Shape Decomposition Based on Discrete Contour Evolution}},
volume = {73},
year = {1999}
}

@article{Latecki1999c,
abstract = {We propose a simple approach to evolution of polygonal curves that is specially designed to fit discrete nature of curves in digital images. It leads to simplification of shape complexity with no blurring (i.e., shape rounding) effects and no dislocation of relevant features. Moreover, in our approach the problem to determine the size of discrete steps for numerical implementations does not occur, since our evolution method leads in a natural way to a finite number of discrete evolution steps which are just the iterations of a basic procedure of vertex deletion.},
author = {Latecki, Longin Jan and Lak{\"{a}}mper, Rolf},
doi = {10.1007/3-540-48236-9_35},
file = {:C\:/Users/timol/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Latecki, Lak{\"{a}}mper - 1999 - Polygon evolution by vertex deletion.pdf:pdf},
isbn = {354066498X},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Discrete curve evolution,Shape recognition,Shape simplification},
mendeley-groups = {Proposal,BA Thesis},
pages = {398--409},
title = {{Polygon evolution by vertex deletion}},
volume = {1682},
year = {1999}
}

@article{Nehring2021,
author = {Nehring, Claudia and Gaiser, Carmen},
journal = {Verband Unbemannte Luftfahrt (VUL): Drone Industry Insights},
mendeley-groups = {Proposal,Proposal/Small},
pages = {23},
title = {{Analyse des deutschen Drohnenmarktes}},
url = {https://www.bdl.aero/de/publikation/analyse-des-deutschen-drohnenmarktes/  https://www.bdli.de/sites/default/files/global\_upload\_upload/Analyse des deutschen Drohnenmarktes.pdf},
year = {2021}
}

@misc{futuretrends2017,
author ={o. A.},
booktitle = {futuretrends.ch},
mendeley-groups = {Proposal/Small},
title = {{Trend: Die Geschichte der Kamera Drohnen}},
url = {https://www.futuretrends.ch/blog/trend-die-geschichte-der-kamera-drohnen/},
urldate = {2023-04-28},
year = {2017}
}

@misc{Schmitt2021,
author = {Schmitt, Florian},
mendeley-groups = {BA Thesis},
title = {{Test Nokia X20 Smartphone - Lange Garantie und 5G - Notebookcheck.com Tests}},
url = {https://www.notebookcheck.com/Test-Nokia-X20-Smartphone-Lange-Garantie-und-5G.536556.0.html#toc-2},
urldate = {2023-09-17},
year = {2021}
}


@article{Otsu1979,
abstract = {A nonparametric and unsupervised method of automatic threshold selection for picture segmentation is presented. An optimal threshold is selected by the discriminant criterion, namely, so as to maximize the separability of the resultant classes in gray levels. The procedure is very simple, utilizing only the zeroth- and the first-order cumulative moments of the gray-level histogram. It is straightforward to extend the method to multithreshold problems. Several experimental results are also presented to support the validity of the method.},
author = {Otsu, Nobuyuki and Smith, P L and Reid, D B and Environment, Cluttered and Palo, Lockheed and Alto, Palo and Smith, P L},
file = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester\_SS\_2023/BA Vorbereitung/Moratz\_Thema/Literatur/A\_Threshold\_Selection\_Method\_from\_Gray-Level\_Histograms.pdf:pdf},
issn = {0018-9472},
journal = {IEEE Transactions on Systems, Man, and Cybernetics},
mendeley-groups = {Proposal/Small},
number = {1},
pages = {62--66},
title = {{A Threshold Selection Method from Gray-Level Histogram}},
volume = {C},
year = {1979}
}
% @article{Redmon2016,
% abstract = {Many applications utilizing Unmanned Aerial Vehicles (UAVs) require the use of computer vision algorithms to analyze the information captured from their on-board camera. Recent advances in deep learning have made it possible to use single-shot Convolutional Neural Network (CNN) detection algorithms that process the input image to detect various objects of interest. To keep the computational demands low these neural networks typically operate on small image sizes which, however, makes it difficult to detect small objects. This is further emphasized when considering UAVs equipped with cameras where due to the viewing range, objects tend to appear relatively small. This paper therefore, explores the trade-offs involved when maintaining the resolution of the objects of interest by extracting smaller patches (tiles) from the larger input image and processing them using a neural network. Specifically, we introduce an attention mechanism to focus on detecting objects only in some of the tiles and a memory mechanism to keep track of information for tiles that are not processed. Through the analysis of different methods and experiments we show that by carefully selecting which tiles to process we can considerably improve the detection accuracy while maintaining comparable performance to CNNs that resize and process a single image which makes the proposed approach suitable for UAV applications.},
% archivePrefix = {arXiv},
% arxivId = {1911.06073},
% author = {Plastiras, George and Kyrkou, Christos and Theocharides, Theocharis},
% doi = {10.1145/3243394.3243692},
% eprint = {1911.06073},
% file = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester_SS_2023/BA Vorbereitung/Moratz_Thema/Literatur/unkategorisiert/YOLO Algorithm Paper.pdf:pdf},
% isbn = {9781450365116},
% journal = {ACM International Conference Proceeding Series},
% keywords = {Aerial cameras,Convolutional neural networks,Object detection,Pedestrian detection},
% mendeley-groups = {Proposal/Small},
% title = {{Efficient convnet-based object detection for unmanned aerial vehicles by selective tile processing}},
% year = {2018}
% }

@incollection{Redmon2016,
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.91},
isbn = {978-1-4673-8852-8},
keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
pages = {779--788},
publisher = {IEEE},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
year = {2016}
}

@misc{Metz2022,
author = {Metz, Joseph},
mendeley-groups = {Proposal/Small},
title = {{Rechtslage: Drohnen an Autobahnen}},
url = {https://www.youtube.com/watch?v=oOw6nVnMbYo},
urldate = {2023-05-15},
year = {2022}
}



@article{Millman2011,
abstract = {Python has arguably become the de facto standard for exploratory, interactive, and computation-driven scientific research. This issue discusses Python's advantages for scientific research and presents several of the core Python libraries and tools used in scientific research. {\textcopyright} 2011 IEEE.},
author = {Millman, K. Jarrod and Aivazis, Michael},
doi = {10.1109/MCSE.2011.36},
file = {:C\:/Users/timol/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Millman, Aivazis - 2011 - Python for scientists and engineers.pdf:pdf},
issn = {15219615},
journal = {Computing in Science and Engineering},
keywords = {Programming languages,Python,Python libraries,Python tools,Scientific computing,interactive research},
mendeley-groups = {BA Thesis},
month = {mar},
number = {2},
pages = {9--12},
title = {{Python for scientists and engineers}},
volume = {13},
year = {2011}
}

@article{Matsugu2003,
abstract = {Reliable detection of ordinary facial expressions (e.g. smile) despite the variability among individuals as well as face appearance is an important step toward the realization of perceptual user interface with autonomous perception of persons. We describe a rule-based algorithm for robust facial expression recognition combined with robust face detection using a convolutional neural network. In this study, we address the problem of subject independence as well as translation, rotation, and scale invariance in the recognition of facial expression. The result shows reliable detection of smiles with recognition rate of 97.6 Percent for 5600 still images of more than 10 subjects. The proposed algorithm demonstrated the ability to discriminate smiling from talking based on the saliency score obtained from voting visual cues. To the best of our knowledge, it is the first facial expression recognition model with the property of subject independence combined with robustness to variability in facial appearance. {\textcopyright} 2003 Elsevier Science Ltd. All rights reserved.},
author = {Matsugu, Masakazu and Mori, Katsuhiko and Mitari, Yusuke and Kaneda, Yuji},
doi = {10.1016/S0893-6080(03)00115-1},
file = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester_SS_2023/BA Vorbereitung/Moratz_Thema/Literatur/YOLO/Vorbemerkungen/Subject-independent-facial-expression-recognition-with-robust-_2003_Neural-N.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Convolutional neural network,Face detection,Facial expression},
mendeley-groups = {BA Thesis},
number = {5-6},
pages = {555--559},
pmid = {12850007},
title = {{Subject independent facial expression recognition with robust face detection using a convolutional neural network}},
volume = {16},
year = {2003}
}


@article{OSheaRyan2022,
abstract = {Abstract: The field of machine learning has taken a dramatic twist in re- cent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models can far exceed the per- performance of previous forms of artificial intelligence in common machine-learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offer a simplified method of getting started with ANNs. This document briefly introduces CNNs, discussing recently published papers and newly formed techniques in developing this bill- leniently fantastic image recognition models. This introduction assumes you are familiar with ANNs and machine learning fundamentals.},
archivePrefix = {arXiv},
arxivId = {1511.08458},
author = {O'Shea, Keiron and Nash, Ryan},
doi = {10.22214/ijraset.2022.47789},
eprint = {1511.08458},
file = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester_SS_2023/BA Vorbereitung/Moratz_Thema/Literatur/YOLO/Vorbemerkungen/An Introduction to Convolutional Neural Networks.pdf:pdf},
journal = {International Journal for Research in Applied Science and Engineering Technology},
keywords = {artificial neural networks,machine learn-,pattern recognition},
mendeley-groups = {BA Thesis},
number = {12},
pages = {943--947},
title = {{An Introduction to Convolutional Neural Networks}},
volume = {10},
year = {2022}
}

@article{Henderson2017,
abstract = {We present a method for training CNN-based object class detectors directly using mean average precision (mAP) as the training loss, in a truly end-to-end fashion that includes non-maximum suppresion (NMS) at training time. This contrasts with the traditional approach of training a CNN for a window classification loss, then applying NMS only at test time, when mAP is used as the evaluation metric in place of classification accuracy. However, mAP following NMS forms a piecewise-constant structured loss over thousands of windows, with gradients that do not convey useful information for gradient descent. Hence, we define new, general gradient-like quantities for piecewise constant functions, which have wide applicability. We describe how to calculate these efficiently for mAP following NMS, enabling to train a detector based on Fast R-CNN [1] directly for mAP. This model achieves equivalent performance to the standard Fast R-CNN on the PASCAL VOC 2007 and 2012 datasets, while being conceptually more appealing as the very same model and loss are used at both training and test time.},
archivePrefix = {arXiv},
arxivId = {1607.03476},
author = {Henderson, Paul and Ferrari, Vittorio},
doi = {10.1007/978-3-319-54193-8_13},
eprint = {1607.03476},
file = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester_SS_2023/BA Vorbereitung/Moratz_Thema/Literatur/YOLO/Vorbemerkungen/End-to-End Training of Object Class Detectors for mean Average Precision.pdf:pdf},
isbn = {9783319541921},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {BA Thesis},
pages = {198--213},
title = {{End-to-end training of object class detectors for mean average precision}},
volume = {10115 LNCS},
year = {2017}
}


@article{Rezatofighi2019,
abstract = {Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that IoU can be directly used as a regression loss. However, IoU has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the this weakness by introducing a generalized version of IoU as both a new loss and a new metric. By incorporating this generalized IoU ( GIoU) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, IoU based, and new, GIoU based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO.},
archivePrefix = {arXiv},
arxivId = {1902.09630},
author = {Rezatofighi, Hamid and Tsoi, Nathan and Gwak, Junyoung and Sadeghian, Amir and Reid, Ian and Savarese, Silvio},
doi = {10.1109/CVPR.2019.00075},
eprint = {1902.09630},
file = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester_SS_2023/BA Vorbereitung/Moratz_Thema/Literatur/YOLO/Vorbemerkungen/Rezatofighi_Generalized_Intersection_Over_Union_A_Metric_and_a_Loss_for_CVPR_2019_paper.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Categorization,Deep Learning,Recognition: Detection,Retrieval},
mendeley-groups = {BA Thesis},
pages = {658--666},
title = {{Generalized intersection over union: A metric and a loss for bounding box regression}},
volume = {2019-June},
year = {2019}
}

@article{Terven2023,
abstract = {YOLO has become a central real-time object detection system for robotics, driverless cars, and video monitoring applications. We present a comprehensive analysis of YOLO's evolution, examining the innovations and contributions in each iteration from the original YOLO to YOLOv8. We start by describing the standard metrics and postprocessing; then, we discuss the major changes in network architecture and training tricks for each model. Finally, we summarize the essential lessons from YOLO's development and provide a perspective on its future, highlighting potential research directions to enhance real-time object detection systems.},
archivePrefix = {arXiv},
arxivId = {2304.00501},
author = {Terven, Juan and Cordova-Esparza, Diana},
eprint = {2304.00501},
file = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester_SS_2023/BA Vorbereitung/Moratz_Thema/Literatur/YOLO/A_Comprehensive_Review_of_YOLO_From_YOLOV1_and_Beyond.pdf:pdf},
keywords = {computer vision,deep learning,object detection,yolo},
mendeley-groups = {BA Thesis},
pages = {1--33},
title = {{A Comprehensive Review of YOLO: From YOLOv1 to YOLOv8 and Beyond}},
url = {http://arxiv.org/abs/2304.00501},
year = {2023}
}

@article{Zheng2020,
abstract = {Bounding box regression is the crucial step in object detection. In existing methods, while ℓn-norm loss is widely adopted for bounding box regression, it is not tailored to the evaluation metric, i.e., Intersection over Union (IoU). Recently, IoU loss and generalized IoU (GIoU) loss have been proposed to benefit the IoU metric, but still suffer from the problems of slow convergence and inaccurate regression. In this paper, we propose a Distance-IoU (DIoU) loss by incorporating the normalized distance between the predicted box and the target box, which converges much faster in training than IoU and GIoU losses. Furthermore, this paper summarizes three geometric factors in bounding box regression, i.e., overlap area, central point distance and aspect ratio, based on which a Complete IoU (CIoU) loss is proposed, thereby leading to faster convergence and better performance. By incorporating DIoU and CIoU losses into state-of-the-art object detection algorithms, e.g., YOLO v3, SSD and Faster R-CNN, we achieve notable performance gains in terms of not only IoU metric but also GIoU metric. Moreover, DIoU can be easily adopted into non-maximum suppression (NMS) to act as the criterion, further boosting performance improvement.},
archivePrefix = {arXiv},
arxivId = {1911.08287},
author = {Zheng, Zhaohui and Wang, Ping and Liu, Wei and Li, Jinze and Ye, Rongguang and Ren, Dongwei},
doi = {10.1609/aaai.v34i07.6999},
eprint = {1911.08287},
file = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester_SS_2023/BA Vorbereitung/Moratz_Thema/Literatur/YOLO/Distance-IoU Loss Faster and Better Learnign for Bounding Box Regression.pdf:pdf},
isbn = {9781577358350},
issn = {2159-5399},
journal = {AAAI 2020 - 34th AAAI Conference on Artificial Intelligence},
mendeley-groups = {BA Thesis},
number = {2},
pages = {12993--13000},
title = {{Distance-IoU loss: Faster and better learning for bounding box regression}},
year = {2020}
}
@article{Li2020,
abstract = {One-stage detector basically formulates object detection as dense classification and localization (i.e., bounding box regression). The classification is usually optimized by Focal Loss and the box location is commonly learned under Dirac delta distribution. A recent trend for one-stage detectors is to introduce an individual prediction branch to estimate the quality of localization, where the predicted quality facilitates the classification to improve detection performance. This paper delves into the representations of the above three fundamental elements: quality estimation, classification and localization. Two problems are discovered in existing practices, including (1) the inconsistent usage of the quality estimation and classification between training and inference, and (2) the inflexible Dirac delta distribution for localization. To address the problems, we design new representations for these elements. Specifically, we merge the quality estimation into the class prediction vector to form a joint representation, and use a vector to represent arbitrary distribution of box locations. The improved representations eliminate the inconsistency risk and accurately depict the flexible distribution in real data, but contain continuous labels, which is beyond the scope of Focal Loss. We then propose Generalized Focal Loss (GFL) that generalizes Focal Loss from its discrete form to the continuous version for successful optimization. On COCO test-dev, GFL achieves 45.0 Percent AP using ResNet-101 backbone, surpassing state-of-the-art SAPD (43.5 Percent) and ATSS (43.6 Percent) with higher or comparable inference speed.},
archivePrefix = {arXiv},
arxivId = {2006.04388},
author = {Li, Xiang and Wang, Wenhai and Wu, Lijun and Chen, Shuo and Hu, Xiaolin and Li, Jun and Tang, Jinhui and Yang, Jian},
eprint = {2006.04388},
file = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester_SS_2023/BA Vorbereitung/Moratz_Thema/Literatur/YOLO/Generalized Focal Loss Learning Qualified and Distributed Bounding Box for Dense Object Detection.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {BA Thesis},
pages = {1--14},
title = {{Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection}},
volume = {2020-December},
year = {2020}
}

@misc{Rossum2009,
author = {Rossum, Guido Van},
mendeley-groups = {Proposal/Small},
title = {{What's New In Python 3.0 — Python v3.0.1 documentation}},
url = {https://docs.python.org/3.0/whatsnew/3.0.html},
urldate = {2023-07-11},
year = {2009}
}

@article{Marowka2018,
abstract = {Python is gaining popularity in academia as the preferred language to teach novices serial programming. The syntax of Python is clean, easy, and simple to understand. At the same time, it is a high-level programming language that supports multi programming paradigms such as imperative, functional, and object-oriented. Therefore, by default, it is almost obvious to believe that Python is also the appropriate language for teaching parallel programming paradigms. This paper presents an in-depth study that examines to what extent Python language is suitable for teaching parallel programming to inexperienced students. The findings show that Python has stumbling blocks that prevent it from preserving its advantages when shifting from serial programming to parallel programming. Therefore, choosing Python as the first language for teaching parallel programming calls for strong justifications, especially when better solutions exist in the community.},
author = {Marowka, Ami},
doi = {10.1007/s10639-017-9607-0},
file = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester_SS_2023/BA Vorbereitung/Moratz_Thema/Literatur/03 Implementierung/Python + externe Bibliotheken/On parallel software engineering education using Python.pdf:pdf},
issn = {15737608},
journal = {Education and Information Technologies},
keywords = {Computer science education,Python,Teaching parallel programming},
mendeley-groups = {BA Thesis},
number = {1},
pages = {357--372},
publisher = {Education and Information Technologies},
title = {{On parallel software engineering education using python}},
volume = {23},
year = {2018}
}


@software{kelsey_jordahl_2020_3946761,
  author       = {Kelsey Jordahl and
                  Joris Van den Bossche and
                  Martin Fleischmann and
                  Jacob Wasserman and
                  James McBride and
                  Jeffrey Gerard and
                  Jeff Tratner and
                  Matthew Perry and
                  Adrian Garcia Badaracco and
                  Carson Farmer and
                  Geir Arne Hjelle and
                  Alan D. Snow and
                  Micah Cochran and
                  Sean Gillies and
                  Lucas Culbertson and
                  Matt Bartos and
                  Nick Eubank and
                  maxalbert and
                  Aleksey Bilogur and
                  Sergio Rey and
                  Christopher Ren and
                  Dani Arribas-Bel and
                  Leah Wasser and
                  Levi John Wolf and
                  Martin Journois and
                  Joshua Wilson and
                  Adam Greenhall and
                  Chris Holdgraf and
                  Filipe and
                  François Leblanc},
  title        = {geopandas/geopandas: v0.8.1},
  month        = jul,
  year         = 2020,
  publisher    = {Zenodo},
  version      = {v0.8.1},
  doi          = {10.5281/zenodo.3946761},
  url          = {https://doi.org/10.5281/zenodo.3946761}
}


@software{reback2020pandas,
    author       = {The pandas development team},
    title        = {pandas-dev/pandas: Pandas},
    month        = feb,
    year         = 2020,
    publisher    = {Zenodo},
    version      = {latest},
    doi          = {10.5281/zenodo.3509134},
    url          = {https://doi.org/10.5281/zenodo.3509134}
}

@misc{numpy_main_web,
mendeley-groups = {BA Thesis},
title = {{NumPy}},
url = {https://numpy.org/},
urldate = {2023-07-11}
}
@misc{numpy_about,
mendeley-groups = {BA Thesis},
title = {{NumPy - About Us}},
url = {https://numpy.org/about/},
urldate = {2023-07-11}
}

@misc{opencv_about,
mendeley-groups = {BA Thesis},
title = {{About - OpenCV}},
url = {https://opencv.org/about/},
urldate = {2023-07-11}
}

@misc{opencv_release,
mendeley-groups = {BA Thesis},
title = {{Releases - OpenCV}},
url = {https://opencv.org/releases/},
urldate = {2023-07-11}
}


@misc{Canu_pysource,
author = {Canu, Sergio},
mendeley-groups = {BA Thesis},
title = {{Instance segmentation YOLO v8 | Opencv with Python tutorial - Pysource}},
url = {https://pysource.com/2023/02/21/instance-segmentation-yolo-v8-opencv-with-python-tutorial/},
urldate = {2023-07-11}
}

@misc{Shui2021,
author = {Shui, Lucien},
mendeley-groups = {BA Thesis},
title = {{timer {\textperiodcentered} PyPI}},
url = {https://pypi.org/project/timer/#history},
urldate = {2023-07-26},
year = {2021}
}
@misc{PSREF21,
mendeley-groups = {BA Thesis},
title = {{PSREF ThinkPad ThinkPad P14s Gen 2 (AMD)}},
url = {https://psref.lenovo.com/Product/ThinkPad_P14s_Gen_2_AMD},
urldate = {2023-07-31}
}

@misc{GMaps,
mendeley-groups = {BA Thesis},
title = {{51°59'45.0"N 7°34'00.1"E - Google Maps}},
url = {https://www.google.de/maps/place/51°59'45.0%22N+7°34'00.1%22E/@51.9958333,7.5666944,17z/data=!3m1!4b1!4m4!3m3!8m2!3d51.9958333!4d7.5666944?entry=ttu},
urldate = {2023-08-02}
}
@misc{davinciresolve,
mendeley-groups = {BA Thesis},
title = {{DaVinci Resolve 18 | Blackmagic Design}},
url = {https://www.blackmagicdesign.com/de/products/davinciresolve},
urldate = {2023-08-02}
}


@article{Latecki2003,
abstract = {Human perception of shape is based on visual parts of objects to a point that a single, significant visual part is sufficient to recognize the whole object. For example, if you see a hand in the door, you expect a human behind the door. Therefore, a cognitively motivated shape similarity measure for recognition applications should be based on visual parts. This cognitive assumption leads to two related problems of scale selection and subpart selection. To find a given query part Q as part of an object C, Q needs to have a correct size with regards to C (scale selection). Assuming that the correct size is selected, the part Q must be compared to all possible subparts of C (subpart selection). For global, contour-based similarity measures, scaling the whole contour curves of both objects to the same length usually solves the problem of scale selection. Although this is not an optimal solution, it works if the whole contour curves are 'sufficiently' similar. Subpart selection problem does not occur in the implementation of global similarity measures. In this paper we present a shape similarity system that is based on correspondence of visual parts, and apply it to robot localization and mapping. This is a particularly interesting application, since the scale selection problem does not occur here and visual parts can be obtained in a very simple way. Therefore, only the problem of subpart selection needs to be solved. Our solution to this problem is based on a contour based shape similarity measure supplemented by a structural arrangement information of visual parts. {\textcopyright} Springer-Verlag Berlin Heidelberg 2003.},
author = {Latecki, Longin Jan and Lak{\"{a}}mper, Rolf and Wolter, Diedrich},
doi = {10.1007/978-3-540-39966-7_3},
file = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester_SS_2023/BA Vorbereitung/Moratz_Thema/Literatur/Shape_Similarity_and_Visual_Parts.pdf:pdf},
isbn = {3540204997},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {BA Thesis},
pages = {34--51},
title = {{Shape similarity and visual parts}},
volume = {2886},
year = {2003}
}


@misc{loss_google,
mendeley-groups = {BA Thesis},
title = {{Descending into ML: Training and Loss  |  Machine Learning  |  Google for Developers}},
url = {https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss?hl=en},
urldate = {2023-08-03}
}


@article{Hosang2017,
abstract = {Object detectors have hugely profited from moving towards an end-to-end learning paradigm: proposals, features, and the classifier becoming one neural network improved results two-fold on general object detection. One indispensable component is non-maximum suppression (NMS), a post-processing algorithm responsible for merging all detections that belong to the same object. The de facto standard NMS algorithm is still fully hand-crafted, suspiciously simple, and - being based on greedy clustering with a fixed distance threshold - forces a trade-off between recall and precision. We propose a new network architecture designed to perform NMS, using only boxes and their score. We report experiments for person detection on PETS and for general object categories on the COCO dataset. Our approach shows promise providing improved localization and occlusion handling.},
archivePrefix = {arXiv},
arxivId = {1705.02950},
author = {Hosang, Jan and Benenson, Rodrigo and Schiele, Bernt},
doi = {10.1109/CVPR.2017.685},
eprint = {1705.02950},
file = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester_SS_2023/BA Vorbereitung/Moratz_Thema/Literatur/Learning non-maximum suppression.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
mendeley-groups = {BA Thesis},
pages = {6469--6477},
title = {{Learning non-maximum suppression}},
volume = {2017-January},
year = {2017}
}


@misc{Ground_truth_desc,
mendeley-groups = {BA Thesis},
title = {{What is Ground Truth - Machine Learning | MLOps Wiki}},
url = {https://censius.ai/wiki/ground-truth},
urldate = {2023-08-03}
}


@misc{SOSQE_desc,
mendeley-groups = {BA Thesis},
title = {{Error Sum of Squares}},
url = {https://hlab.stanford.edu/brian/error_sum_of_squares.html},
urldate = {2023-08-03}
}


@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/S11263-015-0816-Y},
eprint = {1409.0575},
file = {:C\:/Users/timol/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
mendeley-groups = {BA Thesis},
month = {dec},
number = {3},
pages = {211--252},
publisher = {Springer New York LLC},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@misc{imageNET_about,
mendeley-groups = {BA Thesis},
title = {{ImageNet}},
url = {https://www.image-net.org/about.php},
urldate = {2023-08-03}
}

@misc{pascal_voc,
mendeley-groups = {BA Thesis},
title = {{The PASCAL Visual Object Classes Homepage}},
url = {http://host.robots.ox.ac.uk/pascal/VOC/},
urldate = {2023-08-03}
}

@article{Ren2017,
abstract = {Most object detectors contain two important components: a feature extractor and an object classifier. The feature extractor has rapidly evolved with significant research efforts leading to better deep convolutional architectures. The object classifier, however, has not received much attention and many recent systems (like SPPnet and Fast/Faster R-CNN) use simple multi-layer perceptrons. This paper demonstrates that carefully designing deep networks for object classification is just as important. We experiment with region-wise classifier networks that use shared, region-independent convolutional features. We call them 'Networks on Convolutional feature maps' (NoCs). We discover that aside from deep feature maps, a deep and convolutional per-region classifier is of particular importance for object detection, whereas latest superior image classification models (such as ResNets and GoogLeNets) do not directly lead to good detection accuracy without using such a per-region classifier. We show by experiments that despite the effective ResNets and Faster R-CNN systems, the design of NoCs is an essential element for the 1st-place winning entries in ImageNet and MS COCO challenges 2015.},
archivePrefix = {arXiv},
arxivId = {1504.06066},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Zhang, Xiangyu and Sun, Jian},
doi = {10.1109/TPAMI.2016.2601099},
eprint = {1504.06066},
file = {:C\:/Users/timol/OneDrive - Universit{\"{a}}t M{\"{u}}nster/10. Fachsemester_SS_2023/BA Vorbereitung/Moratz_Thema/Literatur/02 Theoretischer Hintergrund/Ren et al.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {CNN,Object detection,convolutional feature map},
mendeley-groups = {BA Thesis},
number = {7},
pages = {1476--1481},
pmid = {27541490},
title = {{Object detection networks on convolutional feature maps}},
volume = {39},
year = {2017}
}

@misc{numpy_angle,
mendeley-groups = {BA Thesis},
title = {{numpy.arctan2 — NumPy v1.25 Manual}},
url = {https://numpy.org/doc/stable/reference/generated/numpy.arctan2.html#numpy.arctan2},
urldate = {2023-08-03}
}
