%!TEX root = ../thesis.tex
\chapter{Objekterkennung mit neuronalen Netzen}
\label{ch:Theoretischer Hintergrund}
% {Die Einführung in den theoretischen Hintergrund dieser Arbeit umfasst die Erläuterung der Aufbereitung der Daten mit einem Maschinellem Lernverfahren, namens YOLO, und eine Einführung in die \glqq Discrete Curve Evolution\grqq{}. Beide Verfahren werden im Rahmen dieser Arbeit miteinander kombiniert.
% }



%\section{\glqq You Only Look Once\grqq{}(YOLO)}
{
	% \section{Grundlagen im Maschinellen Lernen}
	% { 
	% Im Folgenden werden mehrere Begriffe und Abkürzungen für \Ref{subsec:YOLO_Alg} erklärt. \\ \todo{Anführungszeichen checken ' ' "" etc}
	% \begin{figure}[ht]
	% 	\centering
	% 	\includegraphics*[scale = 0.8, keepaspectratio, trim=2 2 2 2 ]{images/YOLO/YOLO_Preamble_Simple_CNN.png}
	% 	\caption[Ein einfaches Convolutional Neural Network]{Ein einfaches CNN, welches aus 5 Layern besteht \citep{OSheaRyan2022}}
	% 	\label{YOLO_simple_CNN}
 	% \end{figure}

	% Ein Bereich im Maschinellem Lernen basiert auf Convolutional Neural Networks (CNN). Diese beschreiben ein künstliches neuronales Netz, welches von biologischen Konzepten beeinflusst wurde und sich gut für Mustererkennung eignet. \citep{Matsugu2003}. 
	% CNNs bestehen aus 3 verschiedenen Arten von Layern, die aufeinander gelegt wurden. Ein einfaches CNN ist in Abb. \Ref{YOLO_simple_CNN} zu sehen. Dessen 4 Grundfunktionalitäten werden im Folgenden näher erläutert. \\
	% Der Input Layer speichert die Pixelwerte des eingegebenen Bildes und leitet diese an den nächsten Layer weiter. Ein Convolutional (CN) Layer berechnet den Output der Neuronen, die mit den lokalen Regionen des Input Layers verbunden sind. Dies geschieht, indem das Skalarprodukt zwischen den Gewichten und Regionen berechnet wird, die mit dem Input verbunden sind. Hier kann auch eine 'Rectified Linear Unit' (ReLu) eingesetzt werden, welche 'elementweise' eine Aktivierungfunktion auf die von der vorherigen Schicht erzeugten Ausgabe angewendet wird \citep{OSheaRyan2022}. \\
	% Ein Pooling Layer verringert dann die Komplexität der Eingabe entlang der räumlichen Dimension, um die Anzahl der Parameter zu reduzieren. Die letzte Schicht ist ein fully-connected Layer, der aus Aktivierungen Werte erzeugt, die für die Klassifizierung verwendet werden können. Hier kann man ebenfalls eine ReLu anwenden, um die Leistung zu verbessern \citep{OSheaRyan2022}. \\

	% 'Mean Average Precision' (mAP) bezeichnet den Mittelwert über die Klassen der interpolierten Average Precision (AP). Diese AP pro Klasse ist die Fläche unter der Precision-/Recall- (PR) Kurve für die detektierten Objekte. %In Abbildung \Ref{YOLO_PR_Curve} ist dies grafisch dargestellt \citep{Henderson2017}. 
	
	% 'Intersection over Union' (IOU) ist eine Metrik zum Vergleich der Ähnlichkeit zwischen zwei beliebigen Formen. Hierfür werden die Maße normalisiert, sodass diese unabhängig von der Größe der zu vergleichenden Objekte sind \citep{Rezatofighi2019}. 

	% Loss bezeichnet einen Wert, der angibt, wie schlecht die Vorhersage eines Modells für ein einzelnes Objekt ist. Dieser Wert ist 0, wenn die Vorhersage perfekt wäre, sonst steigt dieser Wert. Wenn ein Modell trainiert wird, ist das Ziel dieses Trainings, eine Reihe von Gewichten zu finden, die für alle Trainingsdaten im Durchschnitt den geringsten Loss aufweisen. In Abb. \ref{Loss_desc} auf Seite \pageref{Loss_desc} ist ein einfaches Beispiel zu sehen \citep{loss_google}. 

	% Non-Maximum-Supression (NMS) ist ein Algorithmus, der überlappende Boundingboxen, die von Objektdetektoren erzeugt wurden, entfernt. Es werden die Detektionen mit dem geringsten Loss genommen und die nächsten benachbarten Boundingboxen mit höherem Loss gelöscht, wenn diese das gleiche Objekt überlappen \citep{Hosang2017}. 

	% Ground-Truth bezeichnet Daten, die dafür genutzt werden können, die Qualität von Modellen zu überprüfen. Da diese Daten vorab manuell klassifiziert und bewertet wurden, ist bekannt, welches Ergebnis Sie bei der Eingabe in ein Modell liefern müssen \citep{Ground_truth_desc}.

	% Der Sum-Squarred-Error ist die Summe aller Differenzen zwischen jeder Beobachtung und des Gruppenmittelwertes. Dieser kann als Maß zur Variation innerhalb einer Gruppe verwendet werden und ist 0, wenn alle Fälle innerhalb einer Gruppe gleich sind \citep{SOSQE_desc}.

	% PASCAL VOC 2007, 2012 und ImageNet 1000-Class-Competition \citep{Russakovsky2015} sind drei Datensätze, die zum Training von YOLO durch \citeauthor{Redmon2016} verwendet wurden. Der PASCAL VOC Datensatz wurde in der Version von 2007 und 2012 verwendet. Diese Versionen enthalten 20 Klassen, die erkannt werden müssen  \citep{pascal_voc}.  Der ImageNet Datensatz besteht aus ca. 1000 Bilder, die von Menschen annotiert und klassifiziert wurden \citep{imageNET_about}.
	% }
	\section{Der YOLO Algorithmus \label{subsec:YOLO_Alg}} 
	{Da der Blick des Menschen Objekterkennung, -einordnung und -wirkung intuitiv ermöglicht, ist es unserem Gehirn im Zusammenspiel mit unseren Augen möglich, schnell und genau zu sehen. Durch diese Fähigkeiten können wir mit nur wenig bewussten Gedanken komplexe Aufgaben, wie Fahrradfahren bewältigen, bei denen gleichzeitig mehrere Sinne beansprucht werden. \citep{Redmon2016}. \\
	Dem Computer kann dies mit schnellen und genauen Algorithmen zur Objekterkennung beigebracht werden. Aktuelle Systeme nutzen Klassifikatoren zur Objekterkennung. Dieser wird an verschiedenen Stellen in variablen Skalierungen im Testbild angewendet, um eine Klassifizierung eines Objektes zu ermöglichen \citep{Redmon2016}. \\ 
	Außerdem existieren Systeme wie R-CNN, die erst Regionen vorhersagen, um potenzielle Boundingboxes im Bild zu erzeugen, auf welche dann ein Klassifikator angewendet wird. Aufgrund der Einordnung anderer im Bild detektierten Objekte können diese Boundingboxen mit einer Nachprozessierung und durch das Eliminieren doppelter Erkennungen feiner aufgelöst werden. Da jeder Teilschritt einzeln optimiert werden muss, sind diese Systeme sehr langsam und nicht leicht auf Performance zu optimieren. \citep{Redmon2016}. \\
	\begin{figure}[ht]
		\centering
		\includegraphics*[scale = 1, keepaspectratio, trim=2 2 2 2 ]{images/YOLO/YOLO_detection_system.png}
		\caption[Das YOLO Objekterkennungsystem]{Das YOLO Objekterkennungsystem \citep{Redmon2016}}
		\label{YOLO_Objectdetection}
 	\end{figure}\glqq You Only Look Once\grqq{} (YOLO) betrachtet Objekterkennung als einzelnes Regressionsproblem, indem direkt von Bildpixel zu Boundingbox Koordinaten und Klassenwahrscheinlichkeiten berechnet wird. Dieser Algorithmus analysiert nur einmal ein Bild und sagt direkt vorher, welche Objekte wo vorhanden sind. Dadurch ist die Komplexität des  Aufbaus von YOLO sehr gering, wie in Abb. \ref{YOLO_Objectdetection} zu sehen \citep{Redmon2016}. \\
	Ein 'Single Convolutional Neural Network' berechnet gleichzeitig mehrere Boundingboxes und die jeweiligen Klassifizierungswahrscheinlichkeiten. Die Performance zur Objekterkennung wird durch das Training von YOLO mit vollständigen Bildern gesteigert. Durch dieses vereinheitlichte Modell entstehen mehrere Vorteile gegenüber den traditionellen Objekterkennungssystemen \citep{Redmon2016}. \\
	Der erste Vorteil von YOLO ist die gesteigerte Performance. Dies wird dadurch ermöglicht, dass Objekterkennung auf Bildern als Regressionproblem betrachtet wird und deshalb keine komplexe Pipeline die Verarbeitung eines Bildes verlangsamt. YOLO erreicht mehr als das Doppelte an mAP  im Vergleich zu Objekterkennungssystemen (ca. 45 fps) \citep{Redmon2016}. 
	%'Mean Average Precision' (mAP) bezeichnet den Mittelwert über die Klassen der interpolierten Average Precision (AP). Diese AP pro Klasse ist die Fläche unter der Precision-/Recall- (PR) Kurve für die detektierten Objekte. \todo{vielleicht doch auslagern und genau erklären?}\\
	Zweitens analysiert YOLO ein Bild global mit Vorhersagen zur Objekterkennung. Dadurch kann YOLO beim Verwechseln von Hintergrund und Objekten im Vordergrund um die Hälfte im Vergleich zu Fast R-CNN verringern. Dies geschieht vor allem durch den größeren Kontext, den YOLO durch die Gesamtbildanalyse gewinnt \citep{Redmon2016}. \\
	Der dritte Vorteil ist das YOLO mit generalisierten Repräsentationen von Objekten trainiert wurde um die Fehlertoleranz bei der Anwendung auf neue Bereiche und unerwartete Eingaben zu vergrößern, aufgrund der Möglichkeit der hohen Verallgemeinerung \citep{Redmon2016}. \\
	Ein Nachteil von YOLO liegt in der Genauigkeit. Der Algorithmus hat Schwierigkeiten einige, insbesondere kleine, Objekte genau zu lokalisieren \citep{Redmon2016}. \\
	Da der Quellcode, mehrere vortrainierte Modelle und die Trainingsdaten von YOLO Open Source sind und zum Download bereitstehen, ist dieser Algorithmus für den Rahmen dieser Arbeit leicht zugänglich und anwendbar \citep{Redmon2016}. \\

	Die einzelnen Teile der Objektdetektion werden von YOLO in ein neuronales Netz gebündelt. Das Algorithmendesign von YOLO ermöglicht ein 'End to End' Training mit Real-Time-Geschwindigkeit unter hoher durchschnittlicher Genauigkeit, da jede Boundingbox aufgrund von Features im gesamten Bild vorhergesagt wird. Dies wird ebenfalls durch die gleichzeitige Vorhersage aller Boundingboxen für alle Klassen im gesamten Bild unterstützt \citep{Redmon2016}. \\
	YOLO unterteilt in Bild in $S \times S $ Rasterzellen. Wenn der Mittelpunkt eines Objektes in eine Rasterzelle fällt, ist diese für die Erkennung des Objektes zuständig. Boundingboxen und ihre jeweiligen Confidence Scores werden für jede Rasterzelle vorhergesagt \citep{Redmon2016}.
	Der Confidence Score beschreibt, wie sicher sich das Modell ist, dass die Boundingbox ein Objekt dieser Klasse enthält und für wie genau das Modell diese Vorhersage hält. Formal ist dieser folgendermaßen definiert:  $Pr (Object) * IOU^{truth}_{pred}$. Dieser Wert ist 0, wenn kein Objekt in der Zelle existiert. Falls der Wert nicht 0 ist, ist der Confidence Score gleich zur IOU zwischen der vorhergesagten Box und der Ground Truth berechnet. \\
	Jede Boundingbox besteht aus 5 Variablen, die vorhergesagt werden: $x, y, w, h$ und der Confidence. Das Zentrum der Boundingbox wird durch die $(x, y)$ Koordinate dargestellt, welche relativ zu den Grenzen der Rasterzelle ist. $w, h$ (Width, Height) werden relativ zum gesamten Bild berechnet. Die Confidence stellt die IOU zwischen der vorhergesagten Box und einem beliebigen Teil der Ground Truth dar \citep{Redmon2016}. \\
	Jede Rasterzelle berechnet die Anzahl der Klassenwahrscheinlichkeiten $C$ unabhängig von der Anzahl der Boundingboxen $B$. Diese bedingten Klassenwahrscheinlichkeiten $C$, die davon abhängig sind, ob eine Rasterzelle ein Objekt enthält werden mit $Pr(Class_i | Object)$ vorhergesagt \citep{Redmon2016}. \\
	Diese bedingten Klassenwahrscheinlichkeiten $C$ werden zum Testzeitpunkt mit der Confidence der einzelnen Boundingboxen multipliziert, um einen klassenspezifischen Confidence Score für jede Boundingbox zu berechnen. Dazu wird die folgende Formel angewendet: 
	\begin{equation}
	Pr(Class_i|Object) * Pr(Object) * IOU_{pred}^{truth} = Pr(Class_i) * IOU_{pred}^{truth}
	\end{equation}
	Der hier berechnete Wert enthält nicht nur die Wahrscheinlichkeit, dass diese Klasse in der Bounding Box vorkommt, sondern auch wie gut die vorhergesagte Box mit dem detektierten Objekt überein stimmt. Ein Beispielablauf ist in Abbildung \ref{YOLO_Model} zu sehen.
	\begin{figure}[ht]
		\centering
		\includegraphics*[scale = 2, keepaspectratio, trim=2 2 2 2 ]{images/YOLO/YOLO_model.png}
		\caption[Das YOLO Modell]{Das YOLO Modell\citep{Redmon2016}}
		\label{YOLO_Model}
 	\end{figure}

	Abbildung \ref{YOLO_Architecture} zeigt die Architektur von YOLO \citep{Redmon2016}. \\ 
	YOLO ist als 'Convolutional Neural Network' entwickelt worden, welches mit dem PASCAL VOC Datensatz getestet und von GoogleLeNet Modell für die Architektur inspiriert wurde. Die Features des Bildes werden in den ersten CN Layern extrahiert. Die Architektur umfasst 24 'Convolutional' Layer, an welche 2 'fully-connected' Layer anschließen. Außerdem wird ein $1 \times 1$ 'Reduction Layer' gefolgt von $3 \times 3$ 'Convolutional' Layer eingesetzt. Es gibt außerdem eine schnelle Version von YOLO, die ein neuronales Netz mit weniger Layern (9 statt 24) und Filtern verwendet. Abgesehen von diesen Änderungen gleichen sich beide Versionen vollständig in Trainings- und Testparametern \citep{Redmon2016}. \\
	\begin{figure}[h]
		\centering
		\includegraphics*[scale = 1.5, keepaspectratio]{images/YOLO/YOLO_network_arch.png}
		\caption[Die Architektur von YOLO]{Die Architektur von YOLO\citep{Redmon2016}}
		\label{YOLO_Architecture}
 	\end{figure}
	Die finale Vorhersage von YOLO erfolgt in einem $7 \times 7 \times 30$ Tensor \citep{Redmon2016}.
	Die CN Layer wurden auf dem \glqq ImageNet 1000 Class Competition\grqq{} Datensatz trainiert. Das Pretraining wurde mit den ersten 20 CN Layern aus Abb. \ref{YOLO_Architecture}, einem 'Average Pooling' Layer, sowie einem 'fully-connected' Layer durchgeführt. Dieses Modell wird danach durch das Hinzufügen von 4 'convoluted' Layern und 2 'fully-connected' Layern mit zufällig initialisierten Gewichten nach \cite{Ren2017}  \citep{Ren2017} verbessert, um eine bessere Performance zu erreichen. Außerdem wird die 'Input Resolution' von $224 \times 224$ auf $448  \times  448$ erhöht, da häufig feinkörnige visuelle Informationen analysiert werden sollen. Die letzte Schicht des Modells prognostiziert Boundingbox Koordinaten und Klassenwahrscheinlichkeiten \citep{Redmon2016}. \\
	Boudingboxbreite und -höhe werden normalisiert, indem diese durch die Bildbreite und -höhe geteilt werden. Dies erreicht, dass beide Werte zwischen 0 und 1 liegen. Außerdem werden die $x$ und $y$ Koordinate der Boundingbox neu berechnet, dass diese als Abstand zu einer bestimmten Rasterzelle lokalisiert werden können. Diese liegen auch zwischen 0 und 1 \citep{Redmon2016}. \\
	Der letzte Layer verwendet eine lineare Aktivierungfunktion. Alle anderen Layer nutzen die folgende Aktivierungfunktion: \todo{Nochmal in Paper gucken}
	\begin{equation}
		\phi(x) = \begin{cases}
			x & \text{if $x > 0$} \\
			0.1x, & \text{otherwise} \\
		\end{cases}
	\end{equation} 
	Im weiteren Verlauf wird der Sum-Squarred-Error aufgrund seiner geringen Komplexität optimiert. Da er jedoch den Klassifizierungsfehler gleich mit dem Lokaliserungsfehler gewichtet gewichtet und viele Rasterzellen in jedem Bild keine Objekte enthalten, kann eine Optimierung die durchschnittliche Genauigkeit lediglich leicht verbessern. Der Confidence Score läuft jedoch in Zellen, die keine Objekte enthalten, gegen 0 und überlagert damit benachbarte Zellen, die Objekte enthalten. Da dies zu einer früheren Abweichung \todo{Was heißt das? nochmal recherchieren} führt, wird dies behoben, indem der Loss der Boundingbox Koordinaten erhöht und der Loss aus dem Confidence Score für die Boxen, die keine Objekte enthalten, verringert wird \citep{Redmon2016}. \\
	Es werden die zwei Parameter $\lambda_{coord}$ und $\lambda_{noobj}$ verwendet und  mit den Werten  $\lambda_{coord} = 5$ und $\lambda_{noobj} = 0.5$ besetzt \citep{Redmon2016}.
	Da der Sum-Squarred-Error Fehler in großen und kleine Bounding Boxen teilweise gleich gewichtet, wird die Quadratwurzel der Breite und Höhe der Bouding Box genutzt \citep{Redmon2016}. \\
	Ein Bounding Box Predictor soll für jedes Objekt möglich sein. Ein Predictor wird  der Vorhersage eines Objektes zugewiesen, abhängig davon welche Vorhersage die höchste IOU mit der Ground Truth hat. Dadurch wird eine Spezialisierung der Bounding Box Predictoren ermöglicht, welches wiederum dazu führt, dass jeder Predictor besser wird bei der Vorhersage von bestimmten, Größen, Seitenverhältnissen und Objektklassen. Dies führt zu einem höheren Gesamterkennungswert des Algorithmus \citep{Redmon2016}. \\
	Die folgende mehrteilige Verlustfunktion wird während des Trainings optimiert (für eine detaillierte Erklärung der einzelnen Elemente siehe Abb. \ref{YOLO_loss_function_detail}):
	\begin{multline}
		\lambda_{coord} \sum_{i=0}^{s^2} \sum_{j=0}^{B} \mathbb{1}_{i j}^{obj}\left[(x_i-\hat{x}_i)^2 + (y_i - \hat{y}_i)^2\right]\\
		+ \lambda_{coord} \sum_{i=0}^{s^2} \sum_{j=0}^{B} \mathbb{1}_{i j}^{obj}\left[\left(\sqrt{w_i} - \sqrt{\hat{w_i}}\right)^2 + \left(\sqrt{h_i}-\sqrt{\hat{h_i}}\right)^2\right]\\
		+ \sum_{i=0}^{s^2} \sum_{j=0}^{B} \mathbb{1}_{i j}^{obj}\left(C_i - \hat{C_i}\right)^2\\
		+ \lambda_{coord} \sum_{i=0}^{s^2} \sum_{j=0}^{B} \mathbb{1}_{i j}^{obj}\left(C_i - \hat{C_i}\right)^2\\
		+ \sum_{i=0}^{s^2} \mathbb{1}_{i j}^{obj} \sum_{c \in classes } \left(p_i(c)-\hat{p_i}(c)\right)^2
	\end{multline} 
	 In dieser Formel steht $\mathbb{1}_{i j}^{obj}$ dafür, ob ein Objekt in einer Rasterzelle $i$ detektiert wurde. $\mathbb{1}_{i j}^{obj}$ bedeutet, dass der $j$-te Boundingbox Predictor in Rasterzelle $i$ diese Vorhersage bewirkt. Es ist zu beachten, dass diese Verlustfunktion den Klassifizierungsfehler nur dann veringert, wenn ein Objekt in der Rasterzelle vorhanden ist. Auch der Bounding Box Koordinatenfehler wird nur dann verringert, wenn dieser Predictor für die Ground Truth der Box, den höchsten IOU von allen Predictoren in dieser Rasterzelle hat \citep{Redmon2016}. \\
	Da jede Gitterzelle nur 2 Bounding Boxen vorhersagen und eine Klasse haben kann, unterliegt YOLO einer räumlichen Einschränkung. Dies begrenzt die Anzahl der benachbarten Objekte, die das Modell vorhersagen kann. Außerdem ist es für das Modell schwierig, kleine Objekte, die in Gruppen auftreten zu detektieren \citep{Redmon2016}. \\
	Eine weitere Herausforderung ist, dass Objekte mit neuen oder ungewöhnlichen Formen auftreten können und dadurch die Vorhersage erschwert wird. Da die Netzwerkarchitektur aus mehreren 'Downsampling' Schichten besteht, benutzt das Modell relativ grobe Features zur Vorhersage der Bounding Boxen \citep{Redmon2016}. \\
	Außerdem sorgt das Training mit einer Verlustfunktion, die die Erkennungsleistung annähert, dafür das Fehler bei kleinen Bounding Boxen genauso wie bei großen Bounding Boxen behandelt. Dies ist ein Nachteil, weil ein kleiner Fehler in einer großen Box meistens wenig Auswirkungen hat, aber ein kleiner Fehler in einer kleinen Box eine sehr viel größer Auswirkung auf die IOU hat. Falsche Lokalisierungen sind eine weitere Hauptfehlerquelle \citep{Redmon2016}. \\
	YOLO ist für den Anwendungszweck dieser Arbeit geeignet, weil der Algorithmus die entsprechende Performance und einfache Verfügbarkeit von trainierten Modellen bietet. Außerdem existieren mehrere weitere Versionen (s. Abb. \ref{YOLO_timeline_vers}, S. \pageref{YOLO_timeline_vers}) von YOLO, die Vorteile in einzelnen Aspekten bieten \citep{Terven2023}. \\

	Im Rahmen dieser Arbeit wird YOLOv8 von Ultralytics verwendet. Diese bietet Vorteile in der Performance und Genauigkeit der Objektdetektierung.
	} 

	\section{YOLOv8 von Ultralytics}{ \label{subsec:YOLOv8_theoretic}
	
	Im Januar 2023 wurde von der Firma Ultralytics YOLOv8 veröffentlicht, welches auf YOLOv5 basiert. Diese Version beinhaltet 5 verschiedene Modelle, die mit unterschiedlich großen Datensätzen trainiert wurden  \citep{Terven2023}: 
	\begin{itemize}
		\item YOLOv8n (nano)
		\item YOLOv8s (small)
		\item YOLOv8m (medium)
		\item YOLOv8l (large)
		\item YOLOv8x (extra large)
	\end{itemize}
	Ein Vorteil dieser YOLO Implementierung ist, dass verschiedene Varianten für Objektdetektion, -segmentierung und -verfolgung, sowie -klassifizierung existieren. In dieser Arbeit wird hauptsächlich die \grqq -seg\glqq{}-Variante verwendet, welches bereits eine Segmentierung der Umrisse der detektierten Objekte integriert hat.
	\begin{figure}[h]
		\centering
		\includegraphics*[scale = 0.25, keepaspectratio]{images/YOLO/YOLOv8_object_detector_general.png}
		\caption[Architektur modernen Objektdetektoren]{Architektur modernen Objektdetektoren \citep{Terven2023}}
		\label{YOLO_obj_det_gen}
	\end{figure}
	Die Architektur dieser Algorithmen kann man in 3 Teile aufteilen (siehe Abb. \ref{YOLO_obj_det_gen}). Dies sind der Backbone, der Neck und der Head \citep{Terven2023}. \\
	Das Detektieren nützlicher Features vom Eingabebild geschieht im Backbone, welcher meist als CNN implementiert ist. Dieses CNN ist mit einem großen Bilddatensatz, wie ImageNET trainiert worden. Der Backbone detektiert aufeinander aufbauende Features bei unterschiedlichen Skalierungen. Features auf niedrigeren Layern, wie Kanten und Texturen, werden in früheren Schichten im Backbone extrahiert, während Features auf höherer Ebene, wie Objektteile und semantische Informationen, in tieferen Schichten verarbeitet werden \citep{Terven2023}. \\
	Zwischen Backbone und Head wird der Neck eingesetzt, um die Features, die der Backbone ausgibt, zu aggregieren und zu verfeinern. Der Fokus liegt auf der Verbesserung der räumlichen und semantischen Informationen über die unterschiedlichen Skalierungen hinweg. Hier können auch weitere Convolutional Layer eingesetzt werden, um die Darstellung der Features zu verbessern \citep{Terven2023}. \\
	Die letzte Komponente ist der Head, welcher die Vorhersagen, aufgrund der von dem Backbone und Neck gelieferten Features, trifft. Hier werden meistens aufgabenspezifische Teilnetze eingesetzt, um Klassifizierung, Lokalisierung und auch sofortige Segmentierung durchzuführen. Aus den Features, die der Neck liefert, erstellt der Head Vorhersagen für jeden Objektkandidaten. Ein Post-Procressing Schritt, wie die Non-Maximum-Supression (NMS) , filtert überlappende Vorhersagen heraus, sodass nur die sichersten Detektionen genutzt werden \citep{Terven2023}.\\
	Da YOLOv8 auf YOLOv5 basiert, wird in diesem ein ähnlicher Backbone genutzt. Für die Architektur von YOLOv8 siehe Abb. \ref{YOLOv8_Arch} (S. \pageref{YOLOv8_Arch}). Die Änderungen am Backbone betreffen den CSP Layer, welcher jetzt C2f-Modul ('cross-stage partial Bottleneck with two convolutions') genannt wird. Hier werden High-Level-Features mit Kontextinformationen kombiniert, um die Erkennungsgenauigkeit zu verbessern \citep{Terven2023}. \\
	Um unabhängig voneinander Objektivitäts-, Klassifzierungs- und Regressionsaufgaben lösen zu können, benutzt YOLOv8 ein verankerungsfreies Modell mit entkoppeltem Kopf \todo{detaillierter erläutern? Recherchieren!}. Dies ermöglicht, dass sich jeder Branch mit seiner spezifischen Aufgabe beschäftigt und verbessert damit die Gesamtgenauigkeit des Modells \citep{Terven2023}. \\
	Der Output Layer verwendet die Sigmoidfunktion als Aktivierungfunktion für die Objektdetektion, welche die Wahrscheinlichkeit widerspiegelt, dass eine Boundingbox ein Objekt enthält. Die Klassenwahrscheinlichkeiten werden mit der Softmax-Funktion berechnet, welche die Wahrscheinlichkeit der Zugehörigkeit eines Objekts zu jeder möglichen Klasse berechnet \citep{Terven2023}. \\

	Um die Performance insbesondere bei der Objekterkennung von kleineren Objekten zu verbessern, nutzt YOLOv8 CloU \citep{Zheng2020} und DFL \citep{Li2020} Verlustfunktionen für Boundingboxloss und binäre Kreuzentropie für Klassifizierungsloss \citep{Terven2023}. \\

	Mit dem YOLOv8-Seg Modell wird auch eine Variante angeboten, die semantische Segmentierung ermöglicht. Ein CSPDarknet53 Feature Extractor als Backbone wird gefolgt von einem C2f Modul eingesetzt. Diesem C2f Modul sind zwei Header für Segmentierung angeschlossen, die lernen die semantischen Segmentierungsmasken für das Eingabebild vorhersagen. Die Header dieses Modells ähneln denen in  YOLOv8, bestehen aber aus 5 Detektierungsmodulen und einem Prediction Layer. Eine hohe Performance und Effizienz hat dieses Modell bei verschiedenen Benchmarks zur Objekterkennung und semantischen Segmentierung gezeigt \citep{Terven2023}. \\
	Bei einer Evaluation auf dem 'Microsoft COCO-Dataset test-dev 2017' konnte YOLOv8x eine AP von 53,9 \% auf einer Bildgröße von 640 Pixel erreichen. YOLOv5 erreichte bei gleicher Eingabegröße 50,7 \%. Die Geschwindigkeit von YOLOv8x betrug 280 FPS auf einer NVIDIA A100 und TensorRT \citep{Terven2023}.
	}
}
