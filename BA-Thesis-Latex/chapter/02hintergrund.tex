%!TEX root = ../thesis.tex
\chapter{Theoretischer Hintergrund}
\label{ch:Theoretischer Hintergrund}
{Die Einführung in den theoretischen Hintergrund dieser Arbeit umfasst die Erläuterung der Aufbereitung der Daten mit einem Maschinellem Lernverfahren, namens YOLO, und eine Einführung in die \glqq Discrete Curve Evolution\grqq{}.
}



\section{Maschinelles Lernen}
{	Vorbemerkungen hier Erklärung von Abkürzungen etc.
Single Convolutional Network
Klassifizierungswahrscheinlichkeiten
Regressionmodell
kurze andre Objekterkennungssysteme anschneiden ( R CNN etc. ?)?
Pipeline
Mean Average Precision (mAP)
Fast-RCNN
Rasterzelle Statt Gitterzelle benutzen!!
	\subsection{Vorbemerkungen}
	\subsection{You Only Look Once (YOLO)}
	{Da der Blick des Menschen intuitiv Objekterkennung, -einordnung und -wirkung intuitiv ermöglicht, ist es unserem Gehirn im Zusammenspiel mit unseren Augen möglich, schnell und genau zu sehen. Durch diese Fähigkeiten können wir mit nur wenig bewussten Gedanken komplexe Aufgaben, wie Fahrradfahren bewältigen \citep{Plastiras2018}. \\
	Dem Computer kann dies mit schnellen und genauen Algorithmen zur Objekterkennung beigebracht werden. Aktuelle Systeme nutzen Klassifikatoren zur Objekterkennung. Dieser wird an verschiedenen Stellen in variablen Skalierungen im Testbild angewendet, um eine Klassifizierung eines Objektes zu ermöglichen \citep{Plastiras2018}. \\ 
	Außerdem existieren Systeme wie R-CNN, die erst Regionen vorhersagen, um potenzielle Boundingboxes im Bild zu erzeugen, auf welche dann ein Klassifikator angewendet wird. Aufgrund der Einordnung anderer im Bild detektierten Objekte können diese Boundingboxen mit einer Nachprozessierung und durch das Eliminieren doppelter Erkennungen feiner aufgelöst werden. Da jeder Teilschritt einzeln optimiert werden muss, sind diese Systeme sehr langsam und nicht leicht auf Performance zu optimieren. \citep{Plastiras2018}. \\
	\begin{figure}[ht]
		\centering
		\includegraphics*[scale = 1, keepaspectratio, trim=2 2 2 2 ]{images/YOLO/YOLO_detection_system.png}
		\caption[Das YOLO Objekterkennungsystem]{Das YOLO Objekterkennungsystem \citep{Plastiras2018}}
		\label{YOLO_Objecterkennungssystem}
 	\end{figure}
	\glqq You only Look Once\grqq{} (YOLO) betrachtet Objekterkennung als einzelnes Regressionsproblem, indem direkt von Bildpixel zu Boundingbox Koordinaten und Klassenwahrscheinlichkeiten berechnet wird. Dieser Algorithmus analysiert nur einmal ein Bild und sagt direkt vorher, welche Objekte wo vorhanden sind. Dadurch ist die Komplexität des  Aufbaus von YOLO sehr gering, wie in Abb. \ref{YOLO_Objecterkennungssystem} zu sehen \citep{Plastiras2018}. \\
	Ein \glqq Single Convolutional Network\grqq{} berechnet gleichzeitig mehrere Boundingboxes und die jeweiligen Klassifizierungswahrscheinlichkeiten. Die Performance zur Objekterkennung wird durch das Training von YOLO mit vollständigen Bildern gesteigert. Durch dieses vereinheitlichte Modell entstehen mehrere Vorteile gegenüber den traditionellen Objekterkennungssystemen \citep{Plastiras2018}. \\
	Der erste Vorteil von YOLO ist die gesteigerte Performance. Dies wird dadurch ermöglicht, dass Objekterkennung auf Bildern als Regressionproblem betrachtet wird und deshalb keine komplexe Pipeline die Verarbeitung eines Bildes verlangsamt. YOLO erreicht mehr als das Doppelte an mAP  im Vergleich zu Objekterkennungssystemen (ca. 45 fps) \citep{Plastiras2018}. \\
	Zweitens analysiert YOLO ein Bild global mit Vorhersagen zur Objekterkennung. Dadurch kann YOLO beim Verwechseln von Hintergrund und Objekten im Vordergrund um die Hälfte im Vergleich hzu Fast R-CNN veringern. Dies geschieht vor allem durch den größeren Kontext, den YOLO durch die Gesamtbildanalyse gewinnt \citep{Plastiras2018}. \\
	Der dritte Vorteil ist das YOLO mit generalisierten Repräsentationen von Objekten trainiert wurde um die Fehlertoleranz bei der Anwendung auf neue Bereiche und unerwartete Eingaben zu vergrößern, aufgrund der Möglichkeit der hohen Verallgemeinerung \citep{Plastiras2018}. \\
	Ein Nachteil von YOLO liegt in der Genauigkeit. Der Algorithmus hat Schwierigkeiten einige, insbesondere kleine, Objekte genau zu lokalisieren \citep{Plastiras2018}. \\}
	\begin{itemize}
		\item Intro
		\begin{itemize}
			\item Dadurch ist YOLO sehr einfach, wie in Abb. 1 zu sehen. Ein "Single Convolutional Network"(S-CNN) berechnet gleichzeitig mehrere Bounding Boxen und Klassifizierungswahrscheinlichkeiten für diese
			\item YOLO ist mit ganzen Bildern trainiert worden und optimiert dadurch direkt die Performance zur Objekterkennung
			\item Durch das vereinheitlichte Modell entstehen mehrere Vorteile gegenüber den traditionellen Objekterkennungsmethoden
			\item Erstens: Performance; YOLO ist sehr schnell, da Framedetection als Regresssionsproblem verstanden wird, benötigt man keine komplizierte Pipeline. Das Neuronale Netz wird zum Testzeitpunkt auf ein neues Bild angewendet, um Objekte zu erkennen. YOLO erreicht mehr als das Doppelte der durchschnittlichen Genauigkeit anderer Echtzeitobjekterkennungssysteme (Real Time?)
			\item Zweitens: YOLO analysiert ein Bild global mit Vorhersagen zur Objekterkennung. Durch diese Gesamtbildanalyse kann YOLO während der Training- und Testzeit implizit kontextuelle Informationen über die Klassen und deren Aussehen sammeln. Die Anzahl der Fehler von YOLO beim Verwechselnl von Hintergrund und Objekten im Vordergrund ist weniger als halb so hoch wie bei Fast R-CNN, weil YOLO einen größeren Kontext hat
			\item Drittens: YOLO ist mit  generalisierte Repräsentationen von Objekten trainiert. Die Fehlertoleranz ist bei der Anwendung auf neue Bereiche und unerwartete Eingaben sehr viel größer, da es in hohem Maße verallgemeinerbar ist.
			\item Nachteil von YOLO ist die Genauigkeit. Es hat Schwierigkeiten einige Objekte, insbesondere kleinere, genau zu lokalisieren, obwohl es sonst sehr schnell ist.
			\item Die Trainingsdaten und der Quellcode von YOLO sind Open Source. Es sind außerdem mehrere vortrainierte Modelle zum Download verfügbar.
		\end{itemize}
		\item Unified Detection
		\begin{itemize}
			\item YOLO bündelt alle einzelnen Teile der Objektdetektion in ein einzelnes neuronales Netz. Um jede Bounding Box vorherzusagen, benutzt dieser Algorithmus Features vom gesamten Bild. Dadurch das alle Bounding Boxes für alle Klassen im gesamten Bild gleichzeitig vorhergesagt werden, detektiert das Netzwerk global über alle Objekte und im gesamten Bild. Dieses Algorithmendesign ermöglicht ein 'End to End' Training mit Real-Time Geschwindigkeit unter hoher durchschnittlicher Genauigkeit.
			\item Bild wird in $S \times S$ Gitter unterteilt. Eine Gitterzelle ist für die Erkennung des Objektes zuständig, wenn der Mittelpunkt eines Objektes in diese fällt.
			\item $B$ Bounding Boxen und  ihre jeweiligen Confidence Scores werden für jede Gitterzelle vorhergesagt. Der Confidence Score, wie sicher sich das Modell ist, dass die Bounding Box ein Objekt dieser Klasse enthält und für wie genau das Modell diese Vorhersage hält. Dieser Confidence Score ist formal folgendermaßen definiert: FORMEL. Dieser Wert ist 0, wenn kein Objekt in der Zelle existiert. Im anderen Fall ist der Confidence Score gleich zu der 'Intersection over Union' zwischen der vorhergesagten Box (predicted Box) und der Grundwahrheit (Ground Truth)
			\item Jede Bounding Box besteht aus 5 Variablen, die vorhergesagt werden: x,y,w,h und der Confidence. Das Zentrum der Bounding Box wird durch die (x,y) Koordinate dargestellt, welche relativ zu den Grenzen der Gitterzelle sind. (w,h) werden relativ zum gesamten Bild berechnet. Die Confidence stellt, die IOU zwischen der vorhergesagten Box und einem beliebigen Teil der Grundwahrheit dar.
			\item Die durch C bedingten Klassenwahrscheinlichkeiten, welche abhängig davon sind, ob eine Gitterzelle ein Objekt enthält, werden auch durch jede Zelle vorhergesagt: FORMEL. 
			\item Pro Gitterzelle wird die Anzahl der Klassenwahrscheinlichkeiten unabhängig von der Bounding Box Anzahl B berechnet.
			\item Die bedingten Klassenwahrscheinlichkeiten werden zum Testzeitpunkt mit den Confidence Scores der einzelnen Bounding Boxes multipliziert, was einen klassenspezifischen Confidence Score für jede Bounding Box ergibt. Dies geschieht mit folgender FORMEL. Dieser Wert enthält nicht nur die Wahrscheinlichkeit, dass diese Klasse in der Bounding Box vorkommt, sondern auch wie gut das die vorhergesagte Box mit dem detektierten Objekt übereinstimmt. Ein Beispielablauf ist in Abbildung 2 zu sehen
			\item 
			\item 
			\item Network Design
			\begin{itemize}
				\item Das vollständig Netzwerk ist in Abbildung 3 zu sehen. 
				\item Dieses Model ist als CNN implementiert worden. Die Merkmale des Bildes werden in den ersten Schichten der CN Layer des Netzes detektiert. Die (Ausgabe-)Wahrscheinlichkeiten und Koordinaten werden von den 'Fully Connected Layers' vollständig verbundenen Layern generiert.
				\item GoogLeNets Modell für Bildklassifizierung hat die Netzwerkarchitektur von YOLO beeinflusst. Das Netzwerk von YOLO hat 24 Convolutional Layers, an welche 2 'Fully Connected Layers' anschließen.
				\item Es wird ein $1 /times 1$ 'Reduction Layer' gefolgt von  $3 /times 3$ 'Convolutional Layers' eingesetzt.
				\item Außerdem existiert eine schnelle Version von YOLO, die ein neuronales Netz mit weniger Schichten (9 statt 24) und weniger Filtern in diesen verwendet. YOLO und Fast YOLO gleichen sich abgesehen ovn der Größe des Netzwerkes vollständig in den Trainings- und Testparametern.
				\item Die finale Ausgabe dieses Netzes ist ein 7 x 7 x 30 Tensor mit Vorhersagen
			\end{itemize}
			\item Training 
			\begin{itemize}
				\item Die 'Convolutional Layer' wurden auf 'ImageNet 1000-Class Competition' trainiert
				\item Pretraining mit den ersten 20 'Convolutional Layern' aus Abb. 3 und einem 'Average Pooling Layer', sowie einem 'fully connected' Layer
				\item Konvertieren des Modells um die Objekterkennung durchzuführen
				\item es werden nach Ren et al 4 'convoluted layer' und 2 'fully connected layer' mit zufällig initalisierten Gewichten hinzugefügt um die Leistung zu verbessern.  
				\item Die 'Input Resolution' des Netzwerks wird von 224 * 224 auf 448 * 448 erhöht, da häufig feinkörnige visuelle Informationen analysiert werden sollen
				\item letzte Schicht prognostiziert Klassenwahrscheinlichkeiten und Bounding Box Koordinaten
				\begin{itemize}
					\item Bouding Box Breite und Höhe werden normalisiert, indem Sie durch die Bildbreite und -höhe geteilt werden, damit diese zwischen 0 und 1 liegen
					\item x und y Koordinate der Bounding Box werden so neu berechnet, dass Sie als Abstand zu einer bestimmten Gitterzelle lokalisiert werden können. Sie liegen also auch zwischen 0 und 1 
				\end{itemize}
				\item lineare Aktivierungsfunktion für den letzten Layer, alle anderen Layer verwenden folgende Formel FORMEL
				\item Optimierung des Sum-Squared-Errors, da er einfach zu optimieren ist
				\item Stimmt jedoch nicht exakt mit dem Ziel überein, die durchschnittliche Genauigkeit zu erhöhen, da er den Lokalisierungsfehler gleich mit dem Klassifizierungsfehler gewichtet und viele Gitterzellen in jedem Bild keine Objekte enthalten. Aus diesem Grund läuft der Confidence Score in diesen Zellen gegen 0 und überlagert damit benachbarte Zellen, die Objekte enthalten
				\item kann zu einer früheren Abweichung des Trainings führen
				\item Um das zu beheben, wird der Loss der Bounding Box Koordinatenvorhersagen erhöht und der Loss aus den Confidence Score für die Boxen, die keine Objekte enthalten verringert
				\item Zwei Parameter (FORMEL) werden verwendet und mit diesen Werten besetzt um dies zu gewichten
				\item Sum-Squared-Error gewichtet auch Fehler in großen und kleinen Bouding Boxen. Um dies teilweise zu berücksichtigen, dass kleine Fehler in großen Boxen weniger ins Gewicht fallen, wird die Quadratwurzel der Breite und Höhe der Bounding Box genutzt
				\item Yolo kann mehrere Bounding Boxen pro Gitterzelle vorhersagen.
				\item Ein Bounding Box Predictor soll für jedes Objekt möglich sein. Ein Predictor wird  der Vorhersage eines Objektes zugewiesen, abhängig davon welche Vorhersage die höchste IOU mit der Ground Truth hat. Dadurch wird eine Spezialisierung der Bounding Box Predictoren ermöglicht, welches wiederum dazu führt, dass jeder Predictor besser wrid bei der Vorhersage von bestimmten, Größen, Seitenverhältnissen und Objektklassen. Dies führt zu einem höhren Gesamterkennungswert
				\item Die folgende mehrteilige Verlustfunktion wird während des Trainings optimiert: FORMEL
				\item SIEHE PAPER FÜR Erklärung der Faktoren
				\item Es ist zu beachten, dass diese Verlustfunktion den Klassifizierungsfehler nur dann veringert, wenn ein Objekt in der Rasterzelle vorhanden ist. Auch der Bounding Box Koordinatenfehler wird nur dann verringert, wenn dieser Predictor für die Ground Truth der Box, den höchsten IOU von allen Predictoren in dieser Rasterzelle hat.
				\item SIEHE PAPER FÜR WEITERE BESCHREIBUNG VON TRAININGSVERLAUF
			    \end{itemize}
			\item Inference ?[X]
			\item Limitations of YOLO
			\begin{itemize}
				\item Da jede Gitterzelle nur 2 Bounding Boxen vorhersagen und eine Klasse haben kann, unterliegt YOLO einer räumlichen Einschränkung. Dies begrenzt die Anzahl der benachbarten Objekte, die das Modell vorhersagen kann. Außerdem ist es für das Modell schwierig, kleine Objekte, die in Gruppen auftreten zu detektieren. Eine weitere Herausforderung ist, dass Objekte mit neuen oder ungewöhnlichen Formen auftreten können und dadurch die Vorhersage erschwert wird. Da die Netzwerkarchitektur aus mehreren 'Downsampling' Schichten besteht, benutzt das Modell relativ grobe Features zur Vorhersage der Bounding Boxen. Außerdem sorgt das Training mit einer Verlustfunktion, die die Erkennungsleistung annähert, dafür das Fehler bei kleinen Bounding Boxen genauso wie bei großen Bounding Boxen behandelt. Dies ist ein Nachteil, weil ein kleiner Fehler in einer großen Box meistens wenig Auswirkungen hat, aber ein kleiner Fehler in einer kleinen Box eine sehr viel größer Auswirkung auf die IOU hat. Falsche Lokalisierungen sind eine weitere Hauptfehlerquelle.
			\end{itemize}	

		\end{itemize}
		\item Comparison to Other Detection Systems (X)
		\item Experiments (X)
		\item Real-Time-Detection in the Wild (X)
		\item Conclusion
		Zusammenfassend ist YOLO durch die schnelle Verarbeitung von Eingabedaten sehr gut für den Anwendungsfall dieser Bachelorarbeit geeignet.
	\end{itemize}
	
}



\section{Discrete Curve Evolution (DCE)}
\label{sec:Discrete Curve Evolution}
{Die \glqq Discrete Curve Evolution\grqq{} (DCE, \cite{Latecki1999a,Latecki1999c}) ist eine Methode zur Polygonvereinfachung, die die Formähnlichkeit des Polygons beibehält und 1999 von Longin Latecki und Rolf Lakämper vorgestellt wurde (\citep{Barkowsky2000,Latecki1999a,Latecki1999c}). Im Folgenden wird diese Methode genauer erläutert. 
\\
Die Vereinfachung von Polygonen, während die Form der Polygone erkennbar bleibt und kleinere Knicke verschwinden, ist die wichtigste Eigenschaft der DCE. Dies basiert auf der schrittweisen Entfernung von Punkten, die den geringsten Beitrag zur Form des Polygons leisten. Dieser Beitrag des einzelnen Punktes zur Form des Polygons kann in einem Relevanzmaß gemessen werden \citep{Barkowsky2000}. 
\\
\begin{figure}[ht]
	   \centering
	   \includegraphics*[scale = 0.8, keepaspectratio, trim=2 2 2 2 ]{images/schem_maps_paper_kinks.png}
	   \caption[Beispielpolygone für die Erläuterung der Relevanz des Knicks]{Beispielpolygone für die Erläuterung der Relevanz des Knicks. Die fettgedruckten Knicke stellen die betrachteten Liniensegmente dar \citep{Barkowsky2000}.} 
	   \label{Bsp_Rev_Measur_K}
\end{figure}In Abbildung \ref{Bsp_Rev_Measur_K} ist ein Beispiel zu sehen. Bei diesen Formen sind die Knicke durch den Fettdruck zu erkennen. Der Knick in (a) kann als irrelevante Formänderung interpretiert werden, während die Knicke in (b) und (c) deutlich stärker zu erkennen sind. Diese beiden Knicke leisten einen relevanten Beitrag zur Form des Polygons. Der Knick in (d) hat jedoch den größten Anteil an der Form des Beispielobjektes \citep{Barkowsky2000}. 
\\
Diese Unterschiede zum Beitrag eines einzelnen Punktes zur Form eines Polygons lässt sich durch existierende geometrische Konzepte erklären. Wenn man den Knick in Abbildung \ref{Bsp_Rev_Measur_K} (a) mit (b) vergleicht, ist zu erkennen, dass (b) den gleichen Winkel hat wie (a). Der Unterschied ist jedoch, dass die Strecken bei (b) länger sind. Dies erhöht den Beitrag des Punktes in (b) zur Form des Polygons im Vergleich zu dem Punkt in (a).
Der Knick in (c) hat einen größeren Winkel im Vergleich zu (a). Die Länge der Strecken ist jedoch gleich. Bei dem Knick in (d) ist deutlich zu erkennen, dass dieser den signifikantesten Anteil zur Form des Polygons leistet. Dies ist durch den größten Winkel in Verbindung mit den längsten Strecken zwischen Punkten gegeben \citep{Barkowsky2000}.
\\
Dieses Beispiel zeigt, dass die Relevanz jedes Knicks für ein Polygon durch den Winkel und die Länge der an den Punkt anschließenden Liniensegmente definiert werden kann. Je größer der Winkel und die Länge der Liniensegmente sind, desto wichtiger ist der Beitrag des Knicks zur Form der Kurve. Aus diesen Beobachtungen kann eine Funktion K gebildet werden, die den Beitrag eines Knicks zur Form des Polygons misst. Diese sollte monoton steigend sein, wenn die Länge der benachbarten Liniensegmente wächst und der Winkel größer wird \citep{Barkowsky2000}.
\\
Eine formale Definition dieser Funktion kann folgendermaßen erfolgen. Zwei konsekutive Liniensegmente werden als $S_1, S_2$ definiert. Das Maß für die Relevanz des Knicks K, welches aus $S_1 \cup S_2$, dem Winkel  $\beta(S_1, S_2)$ am Scheitelpunkt von $S_1,  S_2$ und den Längen von $S_1, S_2$ besteht, kann nach folgender Formel berechnet werden (nach \citet{Latecki1999a}):
\\
\begin{equation}
	K(S_1,S_2) = \frac{\beta(S_1,S_2)l(S_1)l(S_2)}{l(S_1) + l(S_2)} 
	\label{Equ_K_Bark} 
\end{equation}
 Hier ist $l$ als Funktion definiert, welche die Länge des Segments berechnet. \\
 Der Vorteil dieser Formel ist, dass je höher der Wert von $K(S_1, S_2)$ ist, desto größer ist der Beitrag des Knicks von $S_1 \cup S_2$ zur Form des Polygons \citep{Barkowsky2000}.
 \\
 Nun wird der Prozess der \glqq Discrete Curve Evolution\grqq{} beschrieben.\\ Das Minimum der Kostenfunktion \ref{Equ_K_Bark} ergibt ein Tupel von Liniensegmenten, welches durch eine einzelne Linie ersetzt wird, indem ihre Endpunkte verbunden werden. Dies beschreibt eine Iteration der DCE. Dies wird für jede sich daraus neu ergebene Form wiederholt, indem $K$ für jeden Punkt immer neu berechnet wird \citep{Barkowsky2000}.
 \\ 
 Zusammenfassend ist die DCE folgendermaßen aufgebaut. Der kleinste Wert von $K(S_1,S_2)$ definiert in jedem Iterationsschritt das Paar von konsekutiven Liniensegmenten $S_1, S_2$, welches durch ein einzelnes Liniensegment von den Endpunkten $S_1 \cup S_2$ ersetzt wird. Das Relevanzmaß $K$ wird lokal für jeden Iterationsschritt der DCE neu berechnet und ist deshalb keine lokale Eigenschaft der Form des ursprünglichen Polygons. Dies wird durch die Löschung einiger Liniensegmente im Verlauf der DCE verursacht.\\ Die DCE ermöglicht, wie in Abbildung \ref{Bsp_DCE_Bark_Paper} zu erkennen, die Substitution kleinerer Knicke ohne den Gesamteindruck der Form des Polygons nachhaltig zu verändern \citep{Barkowsky2000}.
 \\
 Ein weiterer Vorteil dieses Algorithmus ist, dass er immer terminiert, da in jedem Iterationsschritt die Zahl der Punkte um eins reduziert wird. Die DCE konvergiert für geschlossene Polygone gegen einen Zustand, wo nur noch drei Liniensegmente im Polygon vorhanden sind. Durch einen Abbruch des Prozesses ist es jedoch möglich, ein Polygon auf eine bestimmte vorgegebenen Punktanzahl zu reduzieren, sodass man ein konvexes Polygon erhält \citep{Barkowsky2000}.
 \begin{figure}[ht]
	   \centering
	   \includegraphics*[scale = 1, keepaspectratio, trim=2 2 2 2 ]{images/schem_maps_paper_DCE.png}
	   \caption[Anwendungsbeispiele für die \glqq Discrete Curve Evolution\grqq{}]{Anwendungsbeispiele für die \glqq Discrete Curve Evolution\grqq{}  \citep{Barkowsky2000}.}
	   \label{Bsp_DCE_Bark_Paper}
\end{figure}

 }

% \blindtext




% \blindmathpaper
% % \Blindtext




% \begin{table}[ht]
% 	\centering
% 	\begin{tabular}{c|c|c}
% 		a & b & c \\ \hline
% 		1 & 2 & 3 \\
% 		1 & 2 & 3 \\
% 		1 & 2 & 3
% 	\end{tabular}
% 	\caption{Eine Tabelle}
% \end{table}

% \begin{figure}[ht]
% 	\centering
% 	\includegraphics[width=0.3\textwidth]{example-image-a}
% 	\caption{Eine Unterschrift}
% \end{figure}

% \begin{figure}[ht]
% 	\centering
% 	\begin{subfigure}[b]{0.45\textwidth}
% 		\includegraphics[width=\textwidth]{example-image-a}
% 		\caption{Eine Unterschrift}
% 	\end{subfigure} \hfill
% 	\begin{subfigure}[b]{0.45\textwidth}
% 		\includegraphics[width=\textwidth]{example-image-b}
% 		\caption{Noch eine Unterschrift}
% 	\end{subfigure}
% 	\caption{Mehr Unterschriften}
% \end{figure}



